---
title: 您的模型组合在无服务器推理中可以运行多快？
---

演讲者: [戚文博](https://github.com/gaius-qi)、[董天欣](https://github.com/FogDong)。

> 视频发布于 2024-9-05.

您是否在为机器学习模型的部署时间慢、运营成本高或可扩展性问题而苦恼？现在，想象一下当典型的人工智能应用程序不仅需要一个模型，
而是一个相互连接的模型套件时所增加的复杂性。在本场演讲中，了解 BentoML 与 Dragonfly 的集成如何有效解决这些挑战，
改变了无服务器 Kubernetes 环境中多模型组合和推理的格局。 加入 BentoML 和 Dragonfly 社区的联合演示，探索一个引人注目的案例研究：一个结合了 LLM、
嵌入和OCR三个模型的RAG应用程序。了解我们的框架不仅高效打包这些多样化的模型，还利用 Dragonfly 创新的 P2P 网络进行快速分发。
我们还将深入探讨其他开源技术，如 JuiceFS 和 VLLM，如何帮助我们实现仅需40秒的部署时间，并为多模型组合部署建立可扩展的蓝图。

<!-- markdownlint-disable -->

<iframe width="720" height="480" src="https://www.youtube.com/embed/aJNIYWDT2mc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen> </iframe>

<!-- markdownlint-restore -->
